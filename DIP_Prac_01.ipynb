{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanvir-Mahamood/Digital-Image-Processing/blob/main/DIP_Prac_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**a) Write a program to read, write and display an image.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0PIQg43SLOLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow  # Only needed in Colab\n",
        "\n",
        "# Read the image\n",
        "image1 = cv2.imread(\"/content/drive/MyDrive/DIP_Image/road.jpg\")\n",
        "\n",
        "# Save the image\n",
        "cv2.imwrite('/content/drive/MyDrive/DIP_Image/road_copy.jpg', image1)\n",
        "\n",
        "# Display the image (use cv2_imshow in Colab)\n",
        "cv2_imshow(image1)"
      ],
      "metadata": {
        "id": "Zq8N4KEhJWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**b) Display the properties of the given image like height, width, no. of channels and size.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JTOV0z3hLjSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original Dimensions : ',image1.shape)\n",
        "height, width, channels = image1.shape\n",
        "size = image1.size\n",
        "\n",
        "# Display the properties\n",
        "print(\"Height:\", height)\n",
        "print(\"Width:\", width)\n",
        "print(\"Number of Channels:\", channels)\n",
        "print(\"Image Size (Total Pixels Ã— Channels):\", size)"
      ],
      "metadata": {
        "id": "iYjg5xwQLovY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**c) Display the individual channels (R, G, B) pixel values in that image.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oT79E0z1MXy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#c\n",
        "print(image1)"
      ],
      "metadata": {
        "id": "CLB_JI_WMZfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**d) Separate the given color image in three R G & B color channels.**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ncUCUcFtNFlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, G, R = cv2.split(image1)\n",
        "k = np.zeros_like(B)\n",
        "\n",
        "B = cv2.merge([B, k, k])\n",
        "G = cv2.merge([k, G, k])\n",
        "R = cv2.merge([k, k, R])\n",
        "\n",
        "# Display each channel\n",
        "print(\"Red Channel:\")\n",
        "cv2_imshow(R)\n",
        "\n",
        "print(\"Green Channel:\")\n",
        "cv2_imshow(G)\n",
        "\n",
        "print(\"Blue Channel:\")\n",
        "cv2_imshow(B)"
      ],
      "metadata": {
        "id": "yI-9LBauNIYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**e) Convert the given color image into gray-scale image and display the shape of the\n",
        "gray-scale image.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vLmsk5eDPdto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray=cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "cv2_imshow(image1)\n",
        "cv2_imshow(gray)\n",
        "print('Gray Image Size : ',gray.shape)"
      ],
      "metadata": {
        "id": "sbzmPxaaPfTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**f) Write a program to read an image and perform cropping and flipping operations\n",
        "on that image.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IlPcIDyzP-uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cropped = image1[100:400, 200:600]\n",
        "\n",
        "# Step 3: Flip the image\n",
        "horizontal_flip = cv2.flip(image1, 1)  # Flip horizontally (left-right)\n",
        "vertical_flip = cv2.flip(image1, 0)    # Flip vertically (up-down)\n",
        "both_flip = cv2.flip(image1, -1)       # Flip both axes\n",
        "\n",
        "# Step 4: Display results\n",
        "print(\"Original Image:\")\n",
        "cv2_imshow(image1)\n",
        "\n",
        "print(\"Cropped Image:\")\n",
        "cv2_imshow(cropped)\n",
        "\n",
        "print(\"Horizontally Flipped Image:\")\n",
        "cv2_imshow(horizontal_flip)\n",
        "\n",
        "print(\"Vertically Flipped Image:\")\n",
        "cv2_imshow(vertical_flip)\n",
        "\n",
        "print(\"Both Axes Flipped Image:\")\n",
        "cv2_imshow(both_flip)"
      ],
      "metadata": {
        "id": "SfrBSIXsQAUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**g) Resize only the width and height of the given image respectively and display both\n",
        "images.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DC65MY1nqUJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resized_image = cv2.resize(image1, (600, 300)) # (width, height)\n",
        "print(\"Original Image\",image1.shape)\n",
        "cv2_imshow(image1)\n",
        "print(\"Resized Image\",resized_image.shape)\n",
        "cv2_imshow(resized_image)"
      ],
      "metadata": {
        "id": "TmpFGTFtqwQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**h) Display the input image by rotating it.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ILhBIJoAt2au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batman = cv2.imread(\"/content/drive/MyDrive/DIP_Image/batman.jpg\")\n",
        "print(\"Original Image: \", batman.shape)\n",
        "cv2_imshow(batman)\n",
        "\n",
        "output1 = cv2.flip(batman, -1)\n",
        "print(\"Output1: \", output1.shape)\n",
        "cv2_imshow(output1)\n",
        "\n",
        "output2 = cv2.rotate(batman, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "print(\"Output2: \", output2.shape)\n",
        "cv2_imshow(output2)\n",
        "\n",
        "output3 = cv2.rotate(batman,cv2.ROTATE_90_CLOCKWISE)\n",
        "print(\"Output3: \", output3.shape)\n",
        "cv2_imshow(output3)"
      ],
      "metadata": {
        "id": "txn4v-wivu3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**i) Write a program to create a 512X512 full black image and draw line, rectangle,\n",
        "circle and text on that image.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "umi5ttbizWSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 512x512 black image (3 channels for RGB)\n",
        "blank_image = np.zeros((512, 512, 3), dtype=np.uint8)\n",
        "\n",
        "# Draw a green diagonal line from top-left to bottom-right\n",
        "cv2.line(blank_image, (0, 0), (512, 512), (10, 255, 10), 2)\n",
        "\n",
        "# Draw a filled blue rectangle (top-left and bottom-right coordinates)\n",
        "cv2.rectangle(blank_image, (200, 20), (400, 220), (255, 0, 0), -1)\n",
        "\n",
        "# Draw a filled red circle (center coordinates and radius)\n",
        "cv2.circle(blank_image, (200, 350), 50, (0, 0, 255), -1)\n",
        "\n",
        "# Add text to the image\n",
        "cv2.putText(\n",
        "    img=blank_image,\n",
        "    text='Hello',\n",
        "    org=(150, 250),\n",
        "    fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
        "    fontScale=1,\n",
        "    color=(0, 255, 0),\n",
        "    thickness=1\n",
        ")\n",
        "\n",
        "# Display the image in Google Colab\n",
        "cv2_imshow(blank_image)\n",
        "print(\"Image size:\", blank_image.shape)"
      ],
      "metadata": {
        "id": "QiFTf70NzYc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**j) Write a program to perform the appropriate operations on the given input image\n",
        "and display the output images.**\n",
        "\n",
        "*   Spatial Resolution Reduction.\n",
        "*   Color Quantization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BglCDGA10GtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/DIP_Image/road.jpg'\n",
        "image_bgr = cv2.imread(image_path)\n",
        "\n",
        "# Convert the image from BGR to RGB for correct color display with Matplotlib\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# --- 2. Part 1: Spatial Resolution Reduction (Downsampling) ---\n",
        "# Define the target sizes for downsampling as seen in the sample output\n",
        "n_values = [334, 112, 38, 13]\n",
        "downsampled_images = []\n",
        "\n",
        "for n in n_values:\n",
        "    # Resize the image. cv2.INTER_AREA is good for shrinking images.\n",
        "    resized_image = cv2.resize(image_rgb, (n, n), interpolation=cv2.INTER_AREA)\n",
        "    downsampled_images.append(resized_image)\n",
        "\n",
        "\n",
        "# --- 3. Part 2: Color Resolution Reduction (K-Means Quantization) ---\n",
        "# Define the number of clusters (colors) as seen in the sample output\n",
        "k_values = [2, 4, 8, 16]\n",
        "quantized_images = []\n",
        "\n",
        "# Prepare the image for K-Means: reshape to a list of pixels and convert to float32\n",
        "pixel_vals = image_rgb.reshape((-1, 3))\n",
        "pixel_vals = np.float32(pixel_vals)\n",
        "\n",
        "for k in k_values:\n",
        "    # Define criteria and apply cv2.kmeans()\n",
        "    # Criteria: 100 iterations or epsilon (accuracy) of 1.0\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 1.0)\n",
        "    # Attempt K-Means clustering 10 times and take the best result\n",
        "    ret, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "    # Convert the center values back to 8-bit integers\n",
        "    centers = np.uint8(centers)\n",
        "\n",
        "    # Create the quantized image by mapping each pixel to its cluster center\n",
        "    segmented_data = centers[labels.flatten()]\n",
        "    quantized_image = segmented_data.reshape((image_rgb.shape))\n",
        "    quantized_images.append(quantized_image)\n",
        "\n",
        "\n",
        "# --- 4. Display the Output Images ---\n",
        "# Create a 2x4 subplot grid to display the results\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "# Display downsampled images on the top row\n",
        "for i, img in enumerate(downsampled_images):\n",
        "    ax = axes[0, i]\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f'N={n_values[i]}')\n",
        "\n",
        "# Display color-quantized images on the bottom row\n",
        "for i, img in enumerate(quantized_images):\n",
        "    ax = axes[1, i]\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f'k={k_values[i]}')\n",
        "\n",
        "# Improve layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QMxjkpLd0Vxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}